{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> PyTorch-ImageSegmentation</h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Human-Segmentation-Dataset-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import cv2\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_FILE='/Addr'\n",
    "DATA_DIR='/content/'\n",
    "DEVICE= 'cuda'\n",
    "\n",
    "EPOCHS=43\n",
    "LR=0.001\n",
    "IMG_SIZE=320\n",
    "BATCH_SIZE=16\n",
    "\n",
    "\n",
    "ENCODER='timm-efficientnet-b0'\n",
    "WEIGHTS='imagenet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(CSV_FILE)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row= df.loc[273]     \n",
    "image_path= row.images\n",
    "mask_path=row.masks\n",
    "\n",
    "image= cv2.imread(image_path)\n",
    "image-cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask=cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE) /255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize = (10,5))\n",
    "\n",
    "ax1.set_title('IMAGE')\n",
    "ax1.imshow(image)\n",
    "\n",
    "ax2.set_title('MASK')\n",
    "ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df= train_test_split(df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "def get_train_augs():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "        A.HorizontalFlip(p= 0.5),\n",
    "        A.VerticalFlip(p= 0.5)\n",
    "    ])\n",
    "def get_valid_augs():\n",
    "    return A.Compose([\n",
    "        A.Resize(IMG_SIZE, IMG_SIZE)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, df, augmentations):\n",
    "        \n",
    "        self.df=df\n",
    "        self.augmentations=augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row= self.df.iloc[idx]\n",
    "\n",
    "        image_path = row.images\n",
    "        mask_path =  row.masks\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = np.expand_dims(mask, axis =-1)\n",
    "\n",
    "        if self.augmentations:\n",
    "            data= self.augmentations(image = image, mask= mask)\n",
    "            image = data['image']\n",
    "            mask = data['mask']\n",
    "\n",
    "\n",
    "        image = np.transpose (image, (2,0,1)).astype(np.float32)  \n",
    "        mask = np.transpose (mask, (2,0,1)).astype(np.float32)  \n",
    "\n",
    "\n",
    "        image = torch. Tensor(image)/ 255.0\n",
    "        mask = torch.round(torch.Tensor(mask)/ 255.0)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = SegmentationDataset(train_df, get_train_augs())\n",
    "validset= SegmentationDataset(valid_df, get_valid_augs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of Trainset  : \", len(trainset))\n",
    "print(\"Size of ValidSet : \", len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=12 \n",
    "\n",
    "image, mask = trainset[idx]\n",
    "helper.show_image(image, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size= BATCH_SIZE, shuffle= True)  \n",
    "validloader = DataLoader(validset, batch_size= BATCH_SIZE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no. of batches in trainloader : \",len(trainloader))\n",
    "print(\"Total no. of batches in validloader : \",len(validloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in trainloader:  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"One batch image shape : \", image.shape, \" <- Batchsize, No. of channel, IMAGE_SIZE, IMAGE_SIZE\")\n",
    "print(\"One batch mask shape : \", mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn  \n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.losses import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "\n",
    "        self.arc = smp.Unet(\n",
    "            encoder_name= ENCODER,\n",
    "            encoder_weights=WEIGHTS,\n",
    "            in_channels= 3,\n",
    "            classes = 1,\n",
    "            activation = None\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, masks =None):\n",
    "        \n",
    "        logits = self.arc(images)\n",
    "\n",
    "        if masks !=None:\n",
    "            loss1= DiceLoss(mode='binary')(logits, masks)   \n",
    "            loss2= nn.BCEWithLogitsLoss()(logits, masks)\n",
    "            return logits, loss1 + loss2\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =SegmentationModel()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "\n",
    "    for images, masks in tqdm(data_loader): \n",
    "        images = images.to(DEVICE)\n",
    "        masks = masks.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(images, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(data_loader, model):\n",
    "    model.eval()\n",
    "    total_loss=0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader):  \n",
    "            \n",
    "            images = images.to(DEVICE)\n",
    "            masks = masks.to(DEVICE)\n",
    "\n",
    "            logits, loss = model(images, masks)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = np.Inf\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    train_loss = train_fn(trainloader, model, optimizer)\n",
    "    valid_loss = eval_fn(validloader, model)\n",
    "\n",
    "if valid_loss < best_valid_loss:   \n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "    print(\"SAVED MODEL\")\n",
    "    best_valid_loss = valid_loss\n",
    "\n",
    "print (\"Epoch : \", i+1,\" Train_loss : \",train_loss,\" Valid_loss : \", valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx= 83\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pt'))  \n",
    "\n",
    "image, mask = validset[idx]\n",
    "\n",
    "logits_mask = model(image.to(DEVICE).unsqueeze(0)) \n",
    "pred_mask = torch.sigmoid(logits_mask)\n",
    "pred_mask = (pred_mask>0.5) *0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper. show_image(image, mask, pred_mask.detach().cpu().squeeze(0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "939b002e18dfaf7d1a3607583eee19df9bce3464fd1a8720950d9657285d0a24"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
